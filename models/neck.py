"""
Feature Refinement Neck æ¨¡å—
æŒ‰ç…§è®ºæ–‡æè¿°å®ç°ç‰¹å¾ç²¾ç‚¼å’ŒèƒŒæ™¯æŠ‘åˆ¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List


class BackgroundSuppressionModule(nn.Module):
    """
    èƒŒæ™¯æŠ‘åˆ¶æ¨¡å—
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ attention æœºåˆ¶ç”Ÿæˆå‰æ™¯/èƒŒæ™¯æ³¨æ„åŠ›å›¾
    2. æŠ‘åˆ¶èƒŒæ™¯åŒºåŸŸçš„ç‰¹å¾å“åº”
    3. å¢å¼ºå‰æ™¯ï¼ˆç›®æ ‡ï¼‰åŒºåŸŸçš„ç‰¹å¾
    
    å‚æ•°ï¼š
        channels (int): è¾“å…¥ç‰¹å¾é€šé“æ•°
        reduction (int): é€šé“å‹ç¼©æ¯”ä¾‹
    """
    
    def __init__(self, channels: int, reduction: int = 4):
        super().__init__()
        self.channels = channels
        
        # æ³¨æ„åŠ›ç”Ÿæˆåˆ†æ”¯
        self.attention = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1),
            nn.BatchNorm2d(channels // reduction),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels // reduction, 3, padding=1),
            nn.BatchNorm2d(channels // reduction),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, 1, 1),
            nn.Sigmoid()  # è¾“å‡º [0, 1] attention map
        )
        
        # ç‰¹å¾å¢å¼ºåˆ†æ”¯
        self.enhance = nn.Sequential(
            nn.Conv2d(channels, channels, 1),
            nn.BatchNorm2d(channels),
            nn.ReLU(inplace=True)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
            x: (B, C, H, W) è¾“å…¥ç‰¹å¾
            
        è¿”å›ï¼š
            out: (B, C, H, W) èƒŒæ™¯æŠ‘åˆ¶åçš„ç‰¹å¾
        """
        # ç”Ÿæˆå‰æ™¯æ³¨æ„åŠ›å›¾
        # é«˜å“åº”åŒºåŸŸ â†’ å‰æ™¯ï¼Œä½å“åº”åŒºåŸŸ â†’ èƒŒæ™¯
        att_map = self.attention(x)  # (B, 1, H, W)
        
        # åŠ æƒç‰¹å¾ï¼ˆæŠ‘åˆ¶èƒŒæ™¯ï¼‰
        x_weighted = x * att_map
        
        # ç‰¹å¾å¢å¼º
        x_enhanced = self.enhance(x_weighted)
        
        # æ®‹å·®è¿æ¥
        out = x + x_enhanced
        
        return out


class FeatureRefinementNeck(nn.Module):
    """
    ç‰¹å¾ç²¾ç‚¼ Neck æ¨¡å—
    
    æŒ‰ç…§è®ºæ–‡ Section 3.3 æè¿°å®ç°ï¼š
    "utilizing upsampling and convolutional layers to refine F into features F0 
    with background suppression mechanism"
    
    åŠŸèƒ½ï¼š
    1. æ¥æ”¶ VPA è¾“å‡ºçš„å¤šå°ºåº¦ç‰¹å¾ [F1, F2, F3, F4]
    2. é€šè¿‡ FPN é£æ ¼çš„èåˆ + ä¸Šé‡‡æ · + å·ç§¯ç²¾ç‚¼
    3. åº”ç”¨èƒŒæ™¯æŠ‘åˆ¶æœºåˆ¶
    4. è¾“å‡ºç²¾ç‚¼åçš„ç‰¹å¾ F0 (H/16, W/16)
    
    å‚æ•°ï¼š
        in_channels_list (List[int]): è¾“å…¥ç‰¹å¾çš„é€šé“æ•°åˆ—è¡¨ [C1, C2, C3, C4]
        out_channels (int): è¾“å‡ºç‰¹å¾é€šé“æ•° C0
        use_background_suppression (bool): æ˜¯å¦ä½¿ç”¨èƒŒæ™¯æŠ‘åˆ¶
    """
    
    def __init__(
        self,
        in_channels_list: List[int],
        out_channels: int = 256,
        use_background_suppression: bool = True
    ):
        super().__init__()
        self.num_levels = len(in_channels_list)
        self.out_channels = out_channels
        
        # 1. Lateral convolutions (1x1 å·ç§¯è°ƒæ•´é€šé“æ•°)
        self.lateral_convs = nn.ModuleList()
        for in_channels in in_channels_list:
            lateral_conv = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, bias=False),
                nn.BatchNorm2d(out_channels)
            )
            self.lateral_convs.append(lateral_conv)
        
        # 2. Refinement convolutions (3x3 å·ç§¯ç²¾ç‚¼ç‰¹å¾)
        self.refine_convs = nn.ModuleList()
        for i in range(self.num_levels):
            refine_conv = nn.Sequential(
                nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )
            self.refine_convs.append(refine_conv)
        
        # 3. Background Suppression Module
        if use_background_suppression:
            self.bg_suppression = BackgroundSuppressionModule(out_channels)
        else:
            self.bg_suppression = None
        
        # 4. Final Refinement (ç”Ÿæˆ F0)
        self.final_refine = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å—æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
            features: List of [F1, F2, F3, F4]
                F1: (B, C1, H/4, W/4)
                F2: (B, C2, H/8, W/8)
                F3: (B, C3, H/16, W/16)
                F4: (B, C4, H/32, W/32)
                
        è¿”å›ï¼š
            F0: (B, C0, H/16, W/16) ç²¾ç‚¼åçš„ç‰¹å¾
        """
        assert len(features) == self.num_levels, \
            f"æœŸæœ› {self.num_levels} ä¸ªç‰¹å¾å±‚ï¼Œå¾—åˆ° {len(features)} ä¸ª"
        
        # Step 1: Lateral convolutions (è°ƒæ•´é€šé“æ•°)
        laterals = []
        for i, (feat, lateral_conv) in enumerate(zip(features, self.lateral_convs)):
            lateral = lateral_conv(feat)
            laterals.append(lateral)
        
        # Step 2: Top-down pathway (è‡ªé¡¶å‘ä¸‹èåˆ)
        # ä»æœ€é«˜å±‚å¼€å§‹ï¼Œé€å±‚ä¸Šé‡‡æ ·å¹¶èåˆ
        for i in range(self.num_levels - 1, 0, -1):
            # ä¸Šé‡‡æ ·é«˜å±‚ç‰¹å¾
            _, _, H, W = laterals[i-1].shape
            laterals[i-1] = laterals[i-1] + F.interpolate(
                laterals[i],
                size=(H, W),
                mode='bilinear',
                align_corners=False
            )
        
        # Step 3: Refinement convolutions (ç²¾ç‚¼æ¯ä¸€å±‚)
        refined = []
        for i, (lateral, refine_conv) in enumerate(zip(laterals, self.refine_convs)):
            refined_feat = refine_conv(lateral)
            refined.append(refined_feat)
        
        # Step 4: é€‰æ‹©ä¸­é—´å±‚ä½œä¸º F0
        # è®ºæ–‡ä¸­ F0 çš„å°ºå¯¸æ˜¯ H/16, å¯¹åº” F3 (index 2)
        F0 = refined[2]  # (B, C0, H/16, W/16)
        
        # Step 5: Background Suppression
        if self.bg_suppression is not None:
            F0 = self.bg_suppression(F0)
        
        # Step 6: Final Refinement
        F0 = self.final_refine(F0)
        
        # ğŸ”¥ Step 7: é¢å¤–ä¸Šé‡‡æ · (stride 16â†’8) ç”¨äºå°ç›®æ ‡æ£€æµ‹
        # åŸå› ï¼š97%çš„ç›®æ ‡ <32Â²åƒç´ ï¼Œstride=16æ—¶ç‰¹å¾å¤ªå°
        # ä¸Šé‡‡æ ·åï¼š640Ã—512 â†’ 80Ã—64ç‰¹å¾å›¾ï¼Œ12pxç›®æ ‡ â†’ 1.5pxç‰¹å¾ âœ…
        F0 = F.interpolate(F0, scale_factor=2, mode='bilinear', align_corners=False)
        # ç°åœ¨ F0: (B, C0, H/8, W/8) instead of (B, C0, H/16, W/16)
        
        return F0


class FeatureRefinementNeckV2(nn.Module):
    """
    ç‰¹å¾ç²¾ç‚¼ Neck æ¨¡å— V2
    
    æ”¹è¿›ç‰ˆæœ¬ï¼šèåˆæ‰€æœ‰å±‚çš„ç‰¹å¾åˆ° H/16 å°ºåº¦
    
    åŠŸèƒ½ï¼š
    1. å°†æ‰€æœ‰ç‰¹å¾ä¸Šé‡‡æ ·/ä¸‹é‡‡æ ·åˆ° H/16 å°ºåº¦
    2. èåˆæ‰€æœ‰å°ºåº¦çš„ä¿¡æ¯
    3. åº”ç”¨èƒŒæ™¯æŠ‘åˆ¶
    4. è¾“å‡º F0
    
    å‚æ•°ï¼š
        in_channels_list (List[int]): è¾“å…¥ç‰¹å¾çš„é€šé“æ•°åˆ—è¡¨
        out_channels (int): è¾“å‡ºç‰¹å¾é€šé“æ•°
        use_background_suppression (bool): æ˜¯å¦ä½¿ç”¨èƒŒæ™¯æŠ‘åˆ¶
    """
    
    def __init__(
        self,
        in_channels_list: List[int],
        out_channels: int = 256,
        use_background_suppression: bool = True
    ):
        super().__init__()
        self.num_levels = len(in_channels_list)
        self.out_channels = out_channels
        
        # è°ƒæ•´æ¯å±‚é€šé“æ•°
        self.adapt_convs = nn.ModuleList()
        for in_channels in in_channels_list:
            adapt_conv = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )
            self.adapt_convs.append(adapt_conv)
        
        # èåˆå·ç§¯
        self.fusion_conv = nn.Sequential(
            nn.Conv2d(out_channels * self.num_levels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        # èƒŒæ™¯æŠ‘åˆ¶
        if use_background_suppression:
            self.bg_suppression = BackgroundSuppressionModule(out_channels)
        else:
            self.bg_suppression = None
        
        # æœ€ç»ˆç²¾ç‚¼
        self.final_refine = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
            features: List of [F1, F2, F3, F4]
                
        è¿”å›ï¼š
            F0: (B, C0, H/16, W/16)
        """
        # ç›®æ ‡å°ºå¯¸ï¼šH/16 (å¯¹åº” F3)
        target_size = features[2].shape[-2:]
        
        # è°ƒæ•´æ‰€æœ‰ç‰¹å¾åˆ°ç›¸åŒå°ºå¯¸å’Œé€šé“æ•°
        adapted = []
        for i, (feat, adapt_conv) in enumerate(zip(features, self.adapt_convs)):
            # è°ƒæ•´é€šé“
            feat = adapt_conv(feat)
            
            # è°ƒæ•´å°ºå¯¸
            if feat.shape[-2:] != target_size:
                feat = F.interpolate(
                    feat,
                    size=target_size,
                    mode='bilinear',
                    align_corners=False
                )
            
            adapted.append(feat)
        
        # èåˆæ‰€æœ‰ç‰¹å¾
        fused = torch.cat(adapted, dim=1)  # (B, C0*4, H/16, W/16)
        F0 = self.fusion_conv(fused)  # (B, C0, H/16, W/16)
        
        # èƒŒæ™¯æŠ‘åˆ¶
        if self.bg_suppression is not None:
            F0 = self.bg_suppression(F0)
        
        # æœ€ç»ˆç²¾ç‚¼
        F0 = self.final_refine(F0)
        
        return F0


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Testing FeatureRefinementNeck...")
    
    # åˆ›å»ºæ¨¡å‹
    neck = FeatureRefinementNeck(
        in_channels_list=[96, 192, 384, 768],
        out_channels=256,
        use_background_suppression=True
    )
    
    # æ¨¡æ‹Ÿè¾“å…¥ç‰¹å¾é‡‘å­—å¡”
    B = 2
    F1 = torch.randn(B, 96, 160, 160)   # H/4
    F2 = torch.randn(B, 192, 80, 80)    # H/8
    F3 = torch.randn(B, 384, 40, 40)    # H/16
    F4 = torch.randn(B, 768, 20, 20)    # H/32
    features = [F1, F2, F3, F4]
    
    print(f"\nè¾“å…¥ç‰¹å¾:")
    for i, feat in enumerate(features):
        print(f"  F{i+1}: {feat.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        F0 = neck(features)
    
    print(f"\nè¾“å‡ºç‰¹å¾:")
    print(f"  F0: {F0.shape}")
    
    # éªŒè¯
    assert F0.shape == (B, 256, 40, 40), f"F0 å½¢çŠ¶é”™è¯¯: {F0.shape}"
    
    print("\nâœ“ FeatureRefinementNeck æµ‹è¯•é€šè¿‡ï¼")
    
    # æµ‹è¯• V2
    print("\n" + "="*60)
    print("Testing FeatureRefinementNeckV2...")
    
    neck_v2 = FeatureRefinementNeckV2(
        in_channels_list=[96, 192, 384, 768],
        out_channels=256,
        use_background_suppression=True
    )
    
    with torch.no_grad():
        F0_v2 = neck_v2(features)
    
    print(f"\nè¾“å‡ºç‰¹å¾ (V2):")
    print(f"  F0: {F0_v2.shape}")
    
    assert F0_v2.shape == (B, 256, 40, 40), f"F0 å½¢çŠ¶é”™è¯¯: {F0_v2.shape}"
    
    print("\nâœ“ FeatureRefinementNeckV2 æµ‹è¯•é€šè¿‡ï¼")
    
    # è®¡ç®—å‚æ•°é‡
    num_params_v1 = sum(p.numel() for p in neck.parameters())
    num_params_v2 = sum(p.numel() for p in neck_v2.parameters())
    
    print(f"\nå‚æ•°é‡å¯¹æ¯”:")
    print(f"  V1: {num_params_v1 / 1e6:.2f}M")
    print(f"  V2: {num_params_v2 / 1e6:.2f}M")
    
    print("\nâœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

