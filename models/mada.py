"""
Motion-Aware Difference Attention (MADA) æ¨¡å—
åŸºäºæ—¶ç©ºå·®åˆ†çš„è¿åŠ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæŠ‘åˆ¶é™æ€èƒŒæ™¯å™ªå£°

æ›¿ä»£åŸæœ‰çš„ Doppler Adaptive Filterï¼Œä½¿ç”¨å¸§å·®è€Œé FFT åˆ†ç¦»è¿åŠ¨ç›®æ ‡

ç®—æ³•åŸç†ï¼š
1. æ—¶åŸŸæ¢¯åº¦è®¡ç®—ï¼šD_pre = |I_t - I_{t-1}|, D_next = |I_{t+1} - I_t|
2. è¿åŠ¨æ˜¾è‘—å›¾ï¼šM_raw = D_pre âŠ™ D_nextï¼ˆå–å‰åå·®åˆ†çš„äº¤é›†ï¼‰
3. æ³¨æ„åŠ›æƒé‡ï¼šA_motion = Ïƒ(F_motion(M_raw))
4. ç‰¹å¾åŠ æƒï¼šI'_t = I_t Â· (1 + Î± Â· A_motion)

ç‰©ç†å…ˆéªŒï¼š
- å°ç›®æ ‡è¿åŠ¨è¿ç»­ï¼ŒèƒŒæ™¯é™æ­¢
- åªæœ‰åœ¨è¿ç»­ä¸¤å¸§éƒ½å­˜åœ¨çš„å˜åŒ–æ‰è¢«è§†ä¸ºå¯é è¿åŠ¨
- æ®‹å·®ç»“æ„ç¡®ä¿é™æ­¢ç›®æ ‡ä¹Ÿèƒ½ä¿ç•™åŸå§‹ç‰¹å¾
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple


class MotionAwareDifferenceAttention(nn.Module):
    """
    Motion-Aware Difference Attention (MADA)

    æ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨å°ç›®æ ‡çš„è¿åŠ¨è¿ç»­æ€§ï¼Œé€šè¿‡å¸§å·®è®¡ç®—åŠ¨æ€æ³¨æ„åŠ›æ©ç 

    ç®—æ³•æµç¨‹ï¼š
    1. è®¡ç®—ç›¸é‚»å¸§çš„ç»å¯¹å·®åˆ†ï¼šD_pre = |I_t - I_{t-1}|, D_next = |I_{t+1} - I_t|
    2. ç”Ÿæˆè¿åŠ¨æ˜¾è‘—å›¾ï¼šM_raw = D_pre âŠ™ D_nextï¼ˆå–å‰åå·®åˆ†çš„äº¤é›†ï¼‰
    3. é€šè¿‡è½»é‡å·ç§¯ç½‘ç»œæ˜ å°„ä¸ºæ³¨æ„åŠ›æƒé‡ï¼šA_motion = Ïƒ(F_motion(M_raw))
    4. ç‰¹å¾åŠ æƒï¼šI'_t = I_t Â· (1 + Î± Â· A_motion)

    ä¼˜åŠ¿ï¼š
    - ä¸ä¾èµ– FFTï¼Œé¿å…é«˜é¢‘èƒŒæ™¯å™ªå£°å¹²æ‰°
    - æ˜¾å¼åˆ©ç”¨æ—¶é—´å·®åˆ†æ•æ‰è¿åŠ¨
    - è®¡ç®—é«˜æ•ˆï¼Œé€‚åˆå®æ—¶æ£€æµ‹

    å‚æ•°ï¼š
        num_frames (int): è¾“å…¥å¸§æ•° Sï¼ˆé€šå¸¸ä¸º3ï¼‰
        in_channels (int): è¾“å…¥é€šé“æ•°ï¼ˆé€šå¸¸ä¸º2ï¼šç°åº¦+çƒ­çº¢å¤–ï¼‰
        alpha (float): åˆå§‹ç¼©æ”¾å› å­ï¼Œå¯å­¦ä¹ 
    """

    def __init__(
        self,
        num_frames: int = 3,
        in_channels: int = 2,
        alpha: float = 0.5
    ):
        super().__init__()
        self.num_frames = num_frames
        self.in_channels = in_channels

        # å¯å­¦ä¹ çš„ç¼©æ”¾å› å­
        self.alpha = nn.Parameter(torch.tensor(alpha))

        # è¿åŠ¨ç‰¹å¾æå–ç½‘ç»œ F_motion
        # è¾“å…¥: è¿åŠ¨æ˜¾è‘—å›¾ (B, C, H, W)
        # è¾“å‡º: æ³¨æ„åŠ›æƒé‡ (B, C, H, W)
        self.motion_net = nn.Sequential(
            nn.Conv2d(in_channels, in_channels * 4, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels * 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels * 4, in_channels * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels * 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels * 2, in_channels, kernel_size=3, padding=1),
            nn.Sigmoid()  # è¾“å‡º [0, 1] çš„æ³¨æ„åŠ›æƒé‡
        )

        # è·¨é€šé“èåˆï¼ˆç”¨äºå¤šé€šé“è¾“å…¥æ—¶å¢å¼ºè¿åŠ¨å“åº”ï¼‰
        self.channel_fusion = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True)
        )

    def compute_temporal_gradient(
        self,
        frames: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—æ—¶åŸŸæ¢¯åº¦ï¼ˆç›¸é‚»å¸§å·®åˆ†ï¼‰

        å‚æ•°ï¼š
            frames: (B, S, C, H, W) è¾“å…¥å¸§åºåˆ—ï¼ŒS é€šå¸¸ä¸º 3

        è¿”å›ï¼š
            D_pre: (B, C, H, W) å‰å‘å·®åˆ† |I_t - I_{t-1}|
            D_next: (B, C, H, W) åå‘å·®åˆ† |I_{t+1} - I_t|
        """
        B, S, C, H, W = frames.shape
        assert S >= 3, f"éœ€è¦è‡³å°‘3å¸§è¾“å…¥ï¼Œå½“å‰ä¸º {S} å¸§"

        # æå– t-1, t, t+1 å¸§ï¼ˆå–ä¸­é—´3å¸§æˆ–æ‰€æœ‰å¸§ï¼‰
        if S == 3:
            I_pre = frames[:, 0]   # (B, C, H, W)
            I_mid = frames[:, 1]   # ä¸­é—´å¸§ t
            I_next = frames[:, 2]
        else:
            # å¦‚æœå¤šäº3å¸§ï¼Œå–ä¸­é—´3å¸§
            mid_idx = S // 2
            I_pre = frames[:, mid_idx - 1]
            I_mid = frames[:, mid_idx]
            I_next = frames[:, mid_idx + 1]

        # è®¡ç®—ç»å¯¹å·®åˆ†
        D_pre = torch.abs(I_mid - I_pre)   # |I_t - I_{t-1}|
        D_next = torch.abs(I_next - I_mid)  # |I_{t+1} - I_t|

        return D_pre, D_next

    def forward(
        self,
        cube: torch.Tensor,
        return_attention: bool = False
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        å‚æ•°ï¼š
            cube: (B, C, H, W, S) Cube å¼ é‡ï¼Œå…¶ä¸­ S ä¸ºæ—¶é—´ç»´åº¦
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›å›¾ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰

        è¿”å›ï¼š
            enhanced_cube: (B, C, H, W, S) å¢å¼ºåçš„ Cube
            æˆ– (enhanced_cube, attention_map) å¦‚æœ return_attention=True
        """
        B, C, H, W, S = cube.shape

        # è½¬æ¢ä¸º (B, S, C, H, W) æ ¼å¼ä»¥ä¾¿å¤„ç†
        frames = cube.permute(0, 4, 1, 2, 3).contiguous()  # (B, S, C, H, W)

        # 1. è®¡ç®—æ—¶åŸŸæ¢¯åº¦
        D_pre, D_next = self.compute_temporal_gradient(frames)  # (B, C, H, W)

        # 2. ç”Ÿæˆè¿åŠ¨æ˜¾è‘—å›¾ï¼ˆå“ˆè¾¾ç›ç§¯ï¼‰
        # åªæœ‰åœ¨è¿ç»­ä¸¤å¸§éƒ½å­˜åœ¨çš„å˜åŒ–æ‰è¢«è§†ä¸ºå¯é è¿åŠ¨
        M_raw = D_pre * D_next  # (B, C, H, W)

        # 3. ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        # å…ˆè¿›è¡Œé€šé“èåˆ
        M_fused = self.channel_fusion(M_raw)  # (B, C, H, W)
        A_motion = self.motion_net(M_fused)   # (B, C, H, W) in [0, 1]

        # 4. è·å–ä¸­é—´å¸§è¿›è¡Œå¢å¼º
        mid_idx = S // 2
        I_mid = frames[:, mid_idx]  # (B, C, H, W)

        # 5. æ®‹å·®å¢å¼ºï¼šI'_t = I_t Â· (1 + Î± Â· A_motion)
        # ä½¿ç”¨ clamp é™åˆ¶ alpha èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
        alpha_clamped = torch.clamp(self.alpha, 0.0, 2.0)
        I_enhanced = I_mid * (1 + alpha_clamped * A_motion)

        # 6. æ›¿æ¢ä¸­é—´å¸§ï¼Œå…¶ä»–å¸§ä¿æŒä¸å˜
        frames_enhanced = frames.clone()
        frames_enhanced[:, mid_idx] = I_enhanced

        # è½¬å›åŸå§‹æ ¼å¼ (B, C, H, W, S)
        cube_enhanced = frames_enhanced.permute(0, 2, 3, 4, 1).contiguous()

        if return_attention:
            return cube_enhanced, A_motion
        return cube_enhanced


class MADALight(nn.Module):
    """
    è½»é‡çº§ MADA æ¨¡å—ï¼ˆç”¨äºå®æ—¶æ£€æµ‹ï¼‰

    ç®€åŒ–ç‰ˆï¼Œå‡å°‘å·ç§¯å±‚æ•°
    """

    def __init__(
        self,
        num_frames: int = 3,
        in_channels: int = 2,
        alpha: float = 0.5
    ):
        super().__init__()
        self.num_frames = num_frames
        self.in_channels = in_channels
        self.alpha = nn.Parameter(torch.tensor(alpha))

        # ç®€åŒ–çš„è¿åŠ¨ç‰¹å¾ç½‘ç»œ
        self.motion_net = nn.Sequential(
            nn.Conv2d(in_channels, in_channels * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels * 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels * 2, in_channels, kernel_size=3, padding=1),
            nn.Sigmoid()
        )

    def forward(self, cube: torch.Tensor) -> torch.Tensor:
        """
        è½»é‡çº§å‰å‘ä¼ æ’­

        å‚æ•°ï¼š
            cube: (B, C, H, W, S)

        è¿”å›ï¼š
            enhanced_cube: (B, C, H, W, S)
        """
        B, C, H, W, S = cube.shape

        # è½¬æ¢æ ¼å¼
        frames = cube.permute(0, 4, 1, 2, 3).contiguous()

        # æå–å¸§
        mid_idx = S // 2
        if S >= 3:
            I_pre = frames[:, mid_idx - 1]
            I_mid = frames[:, mid_idx]
            I_next = frames[:, mid_idx + 1]
        else:
            # å¸§æ•°ä¸è¶³æ—¶ï¼Œè¿”å›åŸå§‹æ•°æ®
            return cube

        # è®¡ç®—è¿åŠ¨æ˜¾è‘—å›¾
        D_pre = torch.abs(I_mid - I_pre)
        D_next = torch.abs(I_next - I_mid)
        M_raw = D_pre * D_next

        # ç”Ÿæˆæ³¨æ„åŠ›å¹¶å¢å¼º
        A_motion = self.motion_net(M_raw)
        alpha_clamped = torch.clamp(self.alpha, 0.0, 2.0)
        I_enhanced = I_mid * (1 + alpha_clamped * A_motion)

        # æ›´æ–°ä¸­é—´å¸§
        frames_enhanced = frames.clone()
        frames_enhanced[:, mid_idx] = I_enhanced

        return frames_enhanced.permute(0, 2, 3, 4, 1).contiguous()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Testing Motion-Aware Difference Attention (MADA)...")

    # æµ‹è¯•åŸºæœ¬ MADA
    mada = MotionAwareDifferenceAttention(num_frames=3, in_channels=2, alpha=0.5)

    # æ¨¡æ‹Ÿè¾“å…¥
    B, C, H, W, S = 2, 2, 640, 640, 3
    cube = torch.randn(B, C, H, W, S)

    print(f"è¾“å…¥ Cube: {cube.shape}")

    # å‰å‘ä¼ æ’­
    cube_enhanced = mada(cube)

    print(f"è¾“å‡ºå¢å¼º Cube: {cube_enhanced.shape}")
    print(f"Î± (å¯å­¦ä¹ ): {mada.alpha.item():.4f}")

    assert cube_enhanced.shape == (B, C, H, W, S), "è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("âœ“ MADA åŸºæœ¬æµ‹è¯•é€šè¿‡ï¼")

    # æµ‹è¯•å¸¦æ³¨æ„åŠ›è¿”å›
    cube_enhanced, attention = mada(cube, return_attention=True)
    print(f"æ³¨æ„åŠ›å›¾: {attention.shape}")

    # æµ‹è¯•è½»é‡çº§ç‰ˆæœ¬
    print("\næµ‹è¯• MADALight...")
    mada_light = MADALight(num_frames=3, in_channels=2, alpha=0.5)
    cube_light = mada_light(cube)
    print(f"è½»é‡çº§è¾“å‡º: {cube_light.shape}")

    print("\nğŸ‰ æ‰€æœ‰ MADA æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
