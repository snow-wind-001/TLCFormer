#!/usr/bin/env python3
"""
è¯„ä¼°æ£€æŸ¥ç‚¹è„šæœ¬
æ¯”è¾ƒæœ€ä½³æƒé‡å’Œæœ€æ–°æƒé‡çš„è¯†åˆ«æ•ˆæœï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import logging
import numpy as np
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from models.osformer import OSFormer
from datasets.rgbt_tiny_coco import RGBTTinyCOCODataset, collate_fn
from torch.utils.data import DataLoader
from utils.visualize import visualize_detection_results, create_visualization


def setup_logger(log_file):
    """è®¾ç½®æ—¥å¿—"""
    logger = logging.getLogger('eval')
    logger.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger


def compute_iou(boxes1, boxes2):
    """
    è®¡ç®—ä¸¤ç»„æ¡†çš„IoU
    boxes1: (N, 4) [x1, y1, x2, y2]
    boxes2: (M, 4) [x1, y1, x2, y2]
    è¿”å›: (N, M) IoUçŸ©é˜µ
    """
    if len(boxes1) == 0 or len(boxes2) == 0:
        return torch.zeros(len(boxes1), len(boxes2))
    
    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
    
    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # (N, M, 2)
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # (N, M, 2)
    
    wh = (rb - lt).clamp(min=0)  # (N, M, 2)
    inter = wh[:, :, 0] * wh[:, :, 1]  # (N, M)
    
    union = area1[:, None] + area2 - inter
    iou = inter / (union + 1e-6)
    
    return iou


def decode_predictions(predictions, img_size, feature_size, score_thresh=0.3):
    """
    è§£ç é¢„æµ‹ç»“æœä¸ºè¾¹ç•Œæ¡†
    
    Returns:
        boxes: (N, 4) [x1, y1, x2, y2]
        scores: (N,)
        labels: (N,)
    """
    cls_pred = predictions['cls']  # (B, num_classes, H, W)
    bbox_pred = predictions['bbox']  # (B, 4, H, W)
    centerness_pred = predictions.get('centerness', None)  # (B, 1, H, W)
    
    B, C, H, W = cls_pred.shape
    device = cls_pred.device
    stride = img_size / feature_size
    
    all_boxes = []
    all_scores = []
    all_labels = []
    
    for b in range(B):
        cls_b = cls_pred[b]  # (num_classes, H, W)
        bbox_b = bbox_pred[b]  # (4, H, W)
        
        scores_b = torch.sigmoid(cls_b)  # (num_classes, H, W)
        
        if centerness_pred is not None:
            centerness_b = torch.sigmoid(centerness_pred[b, 0])  # (H, W)
            scores_b = scores_b * centerness_b.unsqueeze(0)
        
        max_scores, _ = scores_b.max(dim=0)  # (H, W)
        max_labels = scores_b.argmax(dim=0)  # (H, W)
        
        mask = max_scores > score_thresh
        
        if mask.sum() == 0:
            all_boxes.append(torch.zeros(0, 4, device=device))
            all_scores.append(torch.zeros(0, device=device))
            all_labels.append(torch.zeros(0, dtype=torch.long, device=device))
            continue
        
        valid_indices = mask.nonzero(as_tuple=False)  # (N, 2) [h, w]
        valid_scores = max_scores[mask]
        valid_labels = max_labels[mask]
        
        valid_bbox = bbox_pred[b, :, mask].t()  # (N, 4)
        
        h_idx = valid_indices[:, 0].float()
        w_idx = valid_indices[:, 1].float()
        
        center_x = (w_idx + 0.5) * stride
        center_y = (h_idx + 0.5) * stride
        
        l, t, r, b = valid_bbox[:, 0], valid_bbox[:, 1], valid_bbox[:, 2], valid_bbox[:, 3]
        x1 = (center_x - l * stride).clamp(min=0, max=img_size)
        y1 = (center_y - t * stride).clamp(min=0, max=img_size)
        x2 = (center_x + r * stride).clamp(min=0, max=img_size)
        y2 = (center_y + b * stride).clamp(min=0, max=img_size)
        
        boxes_b = torch.stack([x1, y1, x2, y2], dim=1)
        
        all_boxes.append(boxes_b)
        all_scores.append(valid_scores)
        all_labels.append(valid_labels)
    
    return all_boxes, all_scores, all_labels


def calculate_ap(recalls, precisions):
    """è®¡ç®— APï¼ˆä½¿ç”¨ 11-point æ’å€¼ï¼‰"""
    ap = 0.0
    for t in np.arange(0., 1.1, 0.1):
        if np.sum(recalls >= t) == 0:
            p = 0
        else:
            p = np.max(precisions[recalls >= t])
        ap += p / 11.0
    return ap


def evaluate_model(model, dataloader, device, config, class_names, score_thresh=0.3, logger=None):
    """
    è¯¦ç»†è¯„ä¼°æ¨¡å‹
    
    Returns:
        results: dict with detailed metrics
    """
    model.eval()
    
    img_size = config['model']['img_size']
    feature_size = img_size // 16
    
    # å­˜å‚¨æ‰€æœ‰é¢„æµ‹å’ŒGT
    all_predictions = defaultdict(list)  # class_id -> list of (score, matched)
    all_ground_truths = defaultdict(int)  # class_id -> count
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_images': 0,
        'total_predictions': 0,
        'total_ground_truths': 0,
        'predictions_per_image': [],
        'ground_truths_per_image': [],
        'box_sizes': [],  # è®°å½•æ£€æµ‹æ¡†å¤§å°
        'iou_scores': [],  # è®°å½•IoUåˆ†æ•°
    }
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    class_stats = defaultdict(lambda: {
        'tp': 0, 'fp': 0, 'fn': 0,
        'predictions': 0, 'ground_truths': 0,
        'avg_iou': []
    })
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc='Evaluating')):
            rgb = batch['rgb'].to(device)
            thermal = batch['thermal'].to(device)
            targets_batch = batch['targets']
            
            # å‰å‘ä¼ æ’­ï¼ˆå–ä¸­é—´å¸§ï¼‰
            predictions = model(rgb, thermal)
            mid_frame = len(predictions) // 2
            pred = predictions[mid_frame]
            
            # è§£ç é¢„æµ‹
            pred_boxes_list, pred_scores_list, pred_labels_list = decode_predictions(
                pred, img_size, feature_size, score_thresh
            )
            
            # å¤„ç†æ¯ä¸ªæ ·æœ¬
            for b in range(len(targets_batch)):
                target = targets_batch[b]
                pred_boxes = pred_boxes_list[b].cpu()
                pred_scores = pred_scores_list[b].cpu()
                pred_labels = pred_labels_list[b].cpu()
                
                # GT boxes
                if len(target['boxes']) > 0:
                    gt_boxes = torch.tensor(target['boxes'], dtype=torch.float32)
                    gt_labels = torch.tensor(target['labels'], dtype=torch.long)
                    
                    # è½¬æ¢å½’ä¸€åŒ–åæ ‡åˆ°ç»å¯¹åæ ‡
                    gt_boxes[:, [0, 2]] *= img_size
                    gt_boxes[:, [1, 3]] *= img_size
                else:
                    gt_boxes = torch.zeros(0, 4)
                    gt_labels = torch.zeros(0, dtype=torch.long)
                
                stats['total_images'] += 1
                stats['total_predictions'] += len(pred_boxes)
                stats['total_ground_truths'] += len(gt_boxes)
                stats['predictions_per_image'].append(len(pred_boxes))
                stats['ground_truths_per_image'].append(len(gt_boxes))
                
                # è®°å½•æ¡†å¤§å°
                if len(pred_boxes) > 0:
                    box_areas = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])
                    stats['box_sizes'].extend(box_areas.tolist())
                
                # ç»Ÿè®¡æ¯ä¸ªGTæ ‡ç­¾
                for label in gt_labels:
                    all_ground_truths[label.item()] += 1
                    class_stats[label.item()]['ground_truths'] += 1
                
                # ç»Ÿè®¡æ¯ä¸ªé¢„æµ‹
                for label in pred_labels:
                    class_stats[label.item()]['predictions'] += 1
                
                # å¦‚æœæ²¡æœ‰é¢„æµ‹æˆ–GTï¼Œè·³è¿‡åŒ¹é…
                if len(pred_boxes) == 0 or len(gt_boxes) == 0:
                    # æ‰€æœ‰GTéƒ½æ˜¯FN
                    for label in gt_labels:
                        class_stats[label.item()]['fn'] += 1
                    # æ‰€æœ‰é¢„æµ‹éƒ½æ˜¯FP
                    for score, label in zip(pred_scores, pred_labels):
                        all_predictions[label.item()].append((score.item(), False))
                        class_stats[label.item()]['fp'] += 1
                    continue
                
                # è®¡ç®—IoUçŸ©é˜µ
                iou_matrix = compute_iou(pred_boxes, gt_boxes)
                
                # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡ŒåŒ¹é…
                for class_id in range(len(class_names)):
                    # è¯¥ç±»åˆ«çš„é¢„æµ‹
                    pred_mask = pred_labels == class_id
                    pred_class_indices = pred_mask.nonzero(as_tuple=True)[0]
                    
                    # è¯¥ç±»åˆ«çš„GT
                    gt_mask = gt_labels == class_id
                    gt_class_indices = gt_mask.nonzero(as_tuple=True)[0]
                    
                    if len(pred_class_indices) == 0:
                        # æ²¡æœ‰é¢„æµ‹ï¼ŒGTéƒ½æ˜¯FN
                        class_stats[class_id]['fn'] += len(gt_class_indices)
                        continue
                    
                    if len(gt_class_indices) == 0:
                        # æ²¡æœ‰GTï¼Œé¢„æµ‹éƒ½æ˜¯FP
                        for idx in pred_class_indices:
                            all_predictions[class_id].append((pred_scores[idx].item(), False))
                            class_stats[class_id]['fp'] += 1
                        continue
                    
                    # æå–è¯¥ç±»åˆ«çš„IoUå­çŸ©é˜µ
                    iou_sub = iou_matrix[pred_class_indices][:, gt_class_indices]
                    
                    # è´ªå©ªåŒ¹é…ï¼ˆIoU > 0.5ï¼‰
                    matched_gt = set()
                    pred_indices_sorted = torch.argsort(pred_scores[pred_class_indices], descending=True)
                    
                    for i in pred_indices_sorted:
                        pred_idx = pred_class_indices[i]
                        score = pred_scores[pred_idx].item()
                        
                        # æ‰¾åˆ°æœ€å¤§IoUçš„GT
                        ious = iou_sub[i]
                        max_iou, max_gt_idx = ious.max(dim=0)
                        
                        if max_iou >= 0.5 and max_gt_idx.item() not in matched_gt:
                            # TP
                            all_predictions[class_id].append((score, True))
                            matched_gt.add(max_gt_idx.item())
                            class_stats[class_id]['tp'] += 1
                            class_stats[class_id]['avg_iou'].append(max_iou.item())
                            stats['iou_scores'].append(max_iou.item())
                        else:
                            # FP
                            all_predictions[class_id].append((score, False))
                            class_stats[class_id]['fp'] += 1
                    
                    # æœªåŒ¹é…çš„GTæ˜¯FN
                    fn_count = len(gt_class_indices) - len(matched_gt)
                    class_stats[class_id]['fn'] += fn_count
    
    # è®¡ç®— AP å’Œå…¶ä»–æŒ‡æ ‡
    aps = {}
    for class_id in range(len(class_names)):
        predictions = all_predictions[class_id]
        n_gt = all_ground_truths[class_id]
        
        if n_gt == 0:
            aps[class_id] = 0.0
            continue
        
        if len(predictions) == 0:
            aps[class_id] = 0.0
            continue
        
        # æŒ‰åˆ†æ•°æ’åº
        predictions.sort(key=lambda x: x[0], reverse=True)
        
        # è®¡ç®—ç´¯ç§¯TPå’ŒFP
        tp = np.array([1 if matched else 0 for _, matched in predictions])
        fp = np.array([0 if matched else 1 for _, matched in predictions])
        
        tp_cumsum = np.cumsum(tp)
        fp_cumsum = np.cumsum(fp)
        
        recalls = tp_cumsum / n_gt
        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)
        
        # è®¡ç®—AP
        ap = calculate_ap(recalls, precisions)
        aps[class_id] = ap
    
    # è®¡ç®— mAP
    valid_aps = [ap for ap in aps.values() if ap > 0]
    mAP = np.mean(valid_aps) if len(valid_aps) > 0 else 0.0
    
    # è®¡ç®—å…¨å±€Precisionå’ŒRecall
    total_tp = sum(class_stats[i]['tp'] for i in range(len(class_names)))
    total_fp = sum(class_stats[i]['fp'] for i in range(len(class_names)))
    total_fn = sum(class_stats[i]['fn'] for i in range(len(class_names)))
    
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # è®¡ç®—å¹³å‡IoU
    avg_iou = np.mean(stats['iou_scores']) if len(stats['iou_scores']) > 0 else 0.0
    
    # ç»„ç»‡ç»“æœ
    results = {
        'mAP': mAP,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'avg_iou': avg_iou,
        'aps': aps,
        'class_stats': dict(class_stats),
        'stats': stats,
        'total_tp': total_tp,
        'total_fp': total_fp,
        'total_fn': total_fn
    }
    
    return results


def visualize_comparison(results_best, results_latest, class_names, output_dir):
    """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # 1. mAP å¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # æ€»ä½“æŒ‡æ ‡å¯¹æ¯”
    ax = axes[0, 0]
    metrics = ['mAP', 'Precision', 'Recall', 'F1', 'Avg IoU']
    best_values = [results_best['mAP'], results_best['precision'], results_best['recall'], 
                   results_best['f1'], results_best['avg_iou']]
    latest_values = [results_latest['mAP'], results_latest['precision'], results_latest['recall'],
                     results_latest['f1'], results_latest['avg_iou']]
    
    x = np.arange(len(metrics))
    width = 0.35
    ax.bar(x - width/2, best_values, width, label='Best Checkpoint', alpha=0.8)
    ax.bar(x + width/2, latest_values, width, label='Latest Checkpoint', alpha=0.8)
    ax.set_ylabel('Score')
    ax.set_title('Overall Metrics Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # æ¯ç±»åˆ«APå¯¹æ¯”
    ax = axes[0, 1]
    class_ids = list(range(len(class_names)))
    best_aps = [results_best['aps'].get(i, 0) for i in class_ids]
    latest_aps = [results_latest['aps'].get(i, 0) for i in class_ids]
    
    x = np.arange(len(class_names))
    ax.bar(x - width/2, best_aps, width, label='Best Checkpoint', alpha=0.8)
    ax.bar(x + width/2, latest_aps, width, label='Latest Checkpoint', alpha=0.8)
    ax.set_ylabel('AP')
    ax.set_title('Per-Class AP Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(class_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # TP/FP/FN å¯¹æ¯” (Best)
    ax = axes[1, 0]
    best_tp = [results_best['class_stats'][i]['tp'] for i in class_ids]
    best_fp = [results_best['class_stats'][i]['fp'] for i in class_ids]
    best_fn = [results_best['class_stats'][i]['fn'] for i in class_ids]
    
    x = np.arange(len(class_names))
    ax.bar(x - width, best_tp, width, label='TP', alpha=0.8)
    ax.bar(x, best_fp, width, label='FP', alpha=0.8)
    ax.bar(x + width, best_fn, width, label='FN', alpha=0.8)
    ax.set_ylabel('Count')
    ax.set_title('Best Checkpoint: TP/FP/FN per Class')
    ax.set_xticks(x)
    ax.set_xticklabels(class_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # TP/FP/FN å¯¹æ¯” (Latest)
    ax = axes[1, 1]
    latest_tp = [results_latest['class_stats'][i]['tp'] for i in class_ids]
    latest_fp = [results_latest['class_stats'][i]['fp'] for i in class_ids]
    latest_fn = [results_latest['class_stats'][i]['fn'] for i in class_ids]
    
    ax.bar(x - width, latest_tp, width, label='TP', alpha=0.8)
    ax.bar(x, latest_fp, width, label='FP', alpha=0.8)
    ax.bar(x + width, latest_fn, width, label='FN', alpha=0.8)
    ax.set_ylabel('Count')
    ax.set_title('Latest Checkpoint: TP/FP/FN per Class')
    ax.set_xticks(x)
    ax.set_xticklabels(class_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'comparison_charts.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir / 'comparison_charts.png'}")


def generate_report(results_best, results_latest, class_names, output_file, logger):
    """ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("æ£€æŸ¥ç‚¹è¯„ä¼°è¯¦ç»†æŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # æ€»ä½“å¯¹æ¯”
        f.write("### æ€»ä½“æŒ‡æ ‡å¯¹æ¯”\n\n")
        f.write(f"{'Metric':<20} {'Best Checkpoint':<20} {'Latest Checkpoint':<20} {'Difference':<15}\n")
        f.write("-" * 75 + "\n")
        
        metrics = [
            ('mAP', 'mAP'),
            ('Precision', 'precision'),
            ('Recall', 'recall'),
            ('F1 Score', 'f1'),
            ('Avg IoU', 'avg_iou')
        ]
        
        for name, key in metrics:
            best_val = results_best[key]
            latest_val = results_latest[key]
            diff = latest_val - best_val
            diff_str = f"{diff:+.4f}" if diff != 0 else "0.0000"
            f.write(f"{name:<20} {best_val:<20.4f} {latest_val:<20.4f} {diff_str:<15}\n")
        
        f.write("\n")
        
        # æ£€æµ‹ç»Ÿè®¡
        f.write("### æ£€æµ‹ç»Ÿè®¡\n\n")
        f.write(f"{'Statistic':<30} {'Best':<15} {'Latest':<15}\n")
        f.write("-" * 60 + "\n")
        f.write(f"{'Total TP':<30} {results_best['total_tp']:<15} {results_latest['total_tp']:<15}\n")
        f.write(f"{'Total FP':<30} {results_best['total_fp']:<15} {results_latest['total_fp']:<15}\n")
        f.write(f"{'Total FN':<30} {results_best['total_fn']:<15} {results_latest['total_fn']:<15}\n")
        f.write(f"{'Total Images':<30} {results_best['stats']['total_images']:<15} {results_latest['stats']['total_images']:<15}\n")
        f.write(f"{'Total Predictions':<30} {results_best['stats']['total_predictions']:<15} {results_latest['stats']['total_predictions']:<15}\n")
        f.write(f"{'Total Ground Truths':<30} {results_best['stats']['total_ground_truths']:<15} {results_latest['stats']['total_ground_truths']:<15}\n")
        f.write(f"{'Avg Pred/Image':<30} {np.mean(results_best['stats']['predictions_per_image']):<15.2f} {np.mean(results_latest['stats']['predictions_per_image']):<15.2f}\n")
        f.write(f"{'Avg GT/Image':<30} {np.mean(results_best['stats']['ground_truths_per_image']):<15.2f} {np.mean(results_latest['stats']['ground_truths_per_image']):<15.2f}\n")
        
        f.write("\n")
        
        # æ¯ç±»åˆ«è¯¦ç»†ç»Ÿè®¡
        f.write("### æ¯ç±»åˆ«è¯¦ç»†ç»Ÿè®¡\n\n")
        for class_id, class_name in enumerate(class_names):
            f.write(f"#### ç±»åˆ« {class_id}: {class_name}\n\n")
            
            best_stats = results_best['class_stats'][class_id]
            latest_stats = results_latest['class_stats'][class_id]
            best_ap = results_best['aps'].get(class_id, 0)
            latest_ap = results_latest['aps'].get(class_id, 0)
            
            f.write(f"{'Metric':<25} {'Best':<15} {'Latest':<15}\n")
            f.write("-" * 55 + "\n")
            f.write(f"{'AP':<25} {best_ap:<15.4f} {latest_ap:<15.4f}\n")
            f.write(f"{'TP':<25} {best_stats['tp']:<15} {latest_stats['tp']:<15}\n")
            f.write(f"{'FP':<25} {best_stats['fp']:<15} {latest_stats['fp']:<15}\n")
            f.write(f"{'FN':<25} {best_stats['fn']:<15} {latest_stats['fn']:<15}\n")
            f.write(f"{'Predictions':<25} {best_stats['predictions']:<15} {latest_stats['predictions']:<15}\n")
            f.write(f"{'Ground Truths':<25} {best_stats['ground_truths']:<15} {latest_stats['ground_truths']:<15}\n")
            
            if len(best_stats['avg_iou']) > 0:
                f.write(f"{'Avg IoU (matched)':<25} {np.mean(best_stats['avg_iou']):<15.4f} ", end='')
            else:
                f.write(f"{'Avg IoU (matched)':<25} {'N/A':<15} ", end='')
            
            if len(latest_stats['avg_iou']) > 0:
                f.write(f"{np.mean(latest_stats['avg_iou']):<15.4f}\n")
            else:
                f.write(f"{'N/A':<15}\n")
            
            f.write("\n")
        
        # é—®é¢˜åˆ†æ
        f.write("="*80 + "\n")
        f.write("### é—®é¢˜åˆ†æ\n")
        f.write("="*80 + "\n\n")
        
        # 1. mAP ä½çš„åŸå› 
        f.write("#### 1. mAP è¾ƒä½çš„å¯èƒ½åŸå› ï¼š\n\n")
        
        best_mAP = results_best['mAP']
        best_precision = results_best['precision']
        best_recall = results_best['recall']
        
        if best_mAP < 0.3:
            f.write(f"âš ï¸ mAP = {best_mAP:.4f} ç¡®å®è¾ƒä½ï¼Œä¸»è¦é—®é¢˜å¯èƒ½åŒ…æ‹¬ï¼š\n\n")
            
            if best_recall < 0.3:
                f.write(f"1. **å¬å›ç‡è¿‡ä½** (Recall={best_recall:.4f}):\n")
                f.write(f"   - FN æ•°é‡: {results_best['total_fn']} (æ¼æ£€)\n")
                f.write(f"   - æ¨¡å‹å¯èƒ½è¿‡äºä¿å®ˆï¼Œç½®ä¿¡åº¦é˜ˆå€¼å¤ªé«˜\n")
                f.write(f"   - å»ºè®®: é™ä½ score_thresh (å½“å‰ 0.3 â†’ å°è¯• 0.1-0.2)\n\n")
            
            if best_precision < 0.3:
                f.write(f"2. **ç²¾ç¡®åº¦è¿‡ä½** (Precision={best_precision:.4f}):\n")
                f.write(f"   - FP æ•°é‡: {results_best['total_fp']} (è¯¯æ£€)\n")
                f.write(f"   - æ¨¡å‹äº§ç”Ÿå¤ªå¤šä½è´¨é‡æ£€æµ‹\n")
                f.write(f"   - å»ºè®®: æé«˜ score_thresh æˆ–å¢åŠ è®­ç»ƒ\n\n")
            
            # æ£€æŸ¥æ¯ç±»åˆ«AP
            low_ap_classes = [(i, name, results_best['aps'].get(i, 0)) 
                             for i, name in enumerate(class_names) 
                             if results_best['aps'].get(i, 0) < 0.2]
            
            if low_ap_classes:
                f.write(f"3. **æŸäº›ç±»åˆ«è¡¨ç°ç‰¹åˆ«å·®**:\n")
                for class_id, class_name, ap in low_ap_classes:
                    stats = results_best['class_stats'][class_id]
                    f.write(f"   - {class_name}: AP={ap:.4f}, ")
                    f.write(f"TP={stats['tp']}, FP={stats['fp']}, FN={stats['fn']}\n")
                f.write(f"   - å»ºè®®: é’ˆå¯¹è¿™äº›ç±»åˆ«å¢åŠ è®­ç»ƒæ•°æ®æˆ–è°ƒæ•´æŸå¤±æƒé‡\n\n")
            
            # æ£€æŸ¥é¢„æµ‹æ•°é‡
            avg_pred = np.mean(results_best['stats']['predictions_per_image'])
            avg_gt = np.mean(results_best['stats']['ground_truths_per_image'])
            
            if avg_pred < avg_gt * 0.5:
                f.write(f"4. **é¢„æµ‹æ•°é‡è¿‡å°‘**:\n")
                f.write(f"   - å¹³å‡é¢„æµ‹/å›¾åƒ: {avg_pred:.2f}\n")
                f.write(f"   - å¹³å‡GT/å›¾åƒ: {avg_gt:.2f}\n")
                f.write(f"   - æ¨¡å‹äº§ç”Ÿçš„å€™é€‰æ¡†å¤ªå°‘\n")
                f.write(f"   - å»ºè®®: é™ä½ç½®ä¿¡åº¦é˜ˆå€¼æˆ–æ£€æŸ¥æ¨¡å‹è®­ç»ƒ\n\n")
            
            elif avg_pred > avg_gt * 2:
                f.write(f"4. **é¢„æµ‹æ•°é‡è¿‡å¤š**:\n")
                f.write(f"   - å¹³å‡é¢„æµ‹/å›¾åƒ: {avg_pred:.2f}\n")
                f.write(f"   - å¹³å‡GT/å›¾åƒ: {avg_gt:.2f}\n")
                f.write(f"   - æ¨¡å‹äº§ç”Ÿè¿‡å¤šä½è´¨é‡æ£€æµ‹\n")
                f.write(f"   - å»ºè®®: æé«˜ç½®ä¿¡åº¦é˜ˆå€¼æˆ–åŠ å¼ºè®­ç»ƒ\n\n")
        
        # 2. æ£€æŸ¥ç‚¹å¯¹æ¯”åˆ†æ
        f.write("\n#### 2. æœ€ä½³æƒé‡ vs æœ€æ–°æƒé‡ï¼š\n\n")
        
        mAP_diff = results_latest['mAP'] - results_best['mAP']
        
        if abs(mAP_diff) < 0.01:
            f.write("âœ… ä¸¤ä¸ªæ£€æŸ¥ç‚¹æ€§èƒ½å‡ ä¹ç›¸åŒ\n")
            f.write("   - è®­ç»ƒå¯èƒ½å·²ç»æ”¶æ•›\n")
            f.write("   - æˆ–è€…éƒ½å¤„äºè¾ƒå·®çš„å±€éƒ¨æœ€ä¼˜\n\n")
        elif mAP_diff > 0.01:
            f.write(f"âœ… æœ€æ–°æƒé‡æ›´å¥½ (mAP æå‡ {mAP_diff:.4f})\n")
            f.write("   - å»ºè®®ä½¿ç”¨æœ€æ–°æƒé‡\n\n")
        else:
            f.write(f"âš ï¸ æœ€ä½³æƒé‡æ›´å¥½ (mAP ä¸‹é™ {abs(mAP_diff):.4f})\n")
            f.write("   - è®­ç»ƒå¯èƒ½è¿‡æ‹Ÿåˆæˆ–ä¸ç¨³å®š\n")
            f.write("   - å»ºè®®ä½¿ç”¨æœ€ä½³æƒé‡æˆ–é‡æ–°è®­ç»ƒ\n\n")
        
        # 3. æ”¹è¿›å»ºè®®
        f.write("\n#### 3. æ”¹è¿›å»ºè®®ï¼š\n\n")
        
        f.write("**çŸ­æœŸå»ºè®®ï¼ˆè°ƒæ•´æ¨ç†å‚æ•°ï¼‰**:\n")
        f.write("1. è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼:\n")
        f.write("   - å½“å‰: 0.3\n")
        if best_recall < 0.3:
            f.write("   - å»ºè®®å°è¯•: 0.1, 0.15, 0.2 (æé«˜å¬å›ç‡)\n")
        elif best_precision < 0.3:
            f.write("   - å»ºè®®å°è¯•: 0.4, 0.5 (æé«˜ç²¾ç¡®åº¦)\n")
        else:
            f.write("   - å»ºè®®å°è¯•: 0.2-0.4 èŒƒå›´å†…è°ƒæ•´\n")
        f.write("\n")
        
        f.write("2. æ·»åŠ  NMS (éæå¤§å€¼æŠ‘åˆ¶):\n")
        f.write("   - å‡å°‘é‡å¤æ£€æµ‹\n")
        f.write("   - å»ºè®® NMS é˜ˆå€¼: 0.5\n\n")
        
        f.write("**ä¸­æœŸå»ºè®®ï¼ˆé‡æ–°è®­ç»ƒï¼‰**:\n")
        if best_mAP < 0.15:
            f.write("1. æ£€æŸ¥è®­ç»ƒæ˜¯å¦æ­£å¸¸:\n")
            f.write("   - æŸ¥çœ‹æŸå¤±æ›²çº¿æ˜¯å¦ä¸‹é™\n")
            f.write("   - æ£€æŸ¥æ˜¯å¦æœ‰ NaN æˆ–å¼‚å¸¸å€¼\n")
            f.write("   - éªŒè¯æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®\n\n")
        
        f.write("2. è°ƒæ•´è®­ç»ƒè¶…å‚æ•°:\n")
        f.write("   - é™ä½å­¦ä¹ ç‡ (å½“å‰ 1e-4 â†’ 5e-5)\n")
        f.write("   - å¢åŠ è®­ç»ƒè½®æ•°\n")
        f.write("   - è°ƒæ•´æŸå¤±æƒé‡\n\n")
        
        f.write("3. æ•°æ®å¢å¼º:\n")
        f.write("   - å¢åŠ æ•°æ®å¢å¼ºå¼ºåº¦\n")
        f.write("   - æ·»åŠ é’ˆå¯¹å°ç›®æ ‡çš„æ•°æ®å¢å¼º\n\n")
        
        f.write("**é•¿æœŸå»ºè®®ï¼ˆæ¨¡å‹æ”¹è¿›ï¼‰**:\n")
        f.write("1. æ£€æŸ¥æ¨¡å‹æ¶æ„æ˜¯å¦é€‚åˆæ•°æ®é›†\n")
        f.write("2. è€ƒè™‘é¢„è®­ç»ƒæƒé‡\n")
        f.write("3. åˆ†æå¤±è´¥æ¡ˆä¾‹ï¼Œé’ˆå¯¹æ€§æ”¹è¿›\n\n")
    
    logger.info(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + "="*80)
    print("å¿«é€Ÿæ€»ç»“")
    print("="*80)
    print(f"\næœ€ä½³æ£€æŸ¥ç‚¹:")
    print(f"  mAP: {results_best['mAP']:.4f}")
    print(f"  Precision: {results_best['precision']:.4f}")
    print(f"  Recall: {results_best['recall']:.4f}")
    print(f"  F1: {results_best['f1']:.4f}")
    
    print(f"\næœ€æ–°æ£€æŸ¥ç‚¹:")
    print(f"  mAP: {results_latest['mAP']:.4f}")
    print(f"  Precision: {results_latest['precision']:.4f}")
    print(f"  Recall: {results_latest['recall']:.4f}")
    print(f"  F1: {results_latest['f1']:.4f}")
    
    print(f"\nå·®å¼‚:")
    print(f"  mAP: {results_latest['mAP'] - results_best['mAP']:+.4f}")
    print(f"  Precision: {results_latest['precision'] - results_best['precision']:+.4f}")
    print(f"  Recall: {results_latest['recall'] - results_best['recall']:+.4f}")
    
    print("\n" + "="*80)
    print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    print("="*80 + "\n")


def main():
    # é…ç½®
    config_path = './configs/rgbt_tiny_config.yaml'
    checkpoint_dir = './checkpoints/rgbt_tiny'
    output_dir = './evaluation_results'
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(output_dir, f'evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    logger = setup_logger(log_file)
    
    logger.info("="*80)
    logger.info("å¼€å§‹æ£€æŸ¥ç‚¹è¯„ä¼°")
    logger.info("="*80)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    logger.info("åŠ è½½éªŒè¯æ•°æ®é›†...")
    val_dataset = RGBTTinyCOCODataset(
        root_dir=config['data']['root_dir'],
        split='test',
        num_frames=config['model']['num_frames'],
        img_size=config['model']['img_size'],
        modality=config['data']['modality']
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['eval']['batch_size'],
        shuffle=False,
        num_workers=config['data']['num_workers'],
        collate_fn=collate_fn
    )
    
    logger.info(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # ç±»åˆ«åç§°
    class_names = config['classes']['names']
    logger.info(f"ç±»åˆ«æ•°: {len(class_names)}")
    logger.info(f"ç±»åˆ«: {class_names}")
    
    # åŠ è½½æ¨¡å‹
    logger.info("åˆ›å»ºæ¨¡å‹...")
    model = OSFormer(
        num_classes=config['model']['num_classes'],
        num_frames=config['model']['num_frames'],
        img_size=config['model']['img_size']
    ).to(device)
    
    # è¯„ä¼°æœ€ä½³æ£€æŸ¥ç‚¹
    best_checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')
    logger.info(f"\n{'='*80}")
    logger.info(f"è¯„ä¼°æœ€ä½³æ£€æŸ¥ç‚¹: {best_checkpoint_path}")
    logger.info(f"{'='*80}")
    
    if os.path.exists(best_checkpoint_path):
        checkpoint = torch.load(best_checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"âœ… åŠ è½½æœ€ä½³æ£€æŸ¥ç‚¹ (Epoch {checkpoint.get('epoch', 'unknown')})")
        
        results_best = evaluate_model(
            model, val_loader, device, config, class_names,
            score_thresh=0.3, logger=logger
        )
        logger.info(f"æœ€ä½³æ£€æŸ¥ç‚¹ mAP: {results_best['mAP']:.4f}")
    else:
        logger.error(f"âŒ æœ€ä½³æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {best_checkpoint_path}")
        return
    
    # è¯„ä¼°æœ€æ–°æ£€æŸ¥ç‚¹
    latest_checkpoint_path = None
    for epoch in range(100, 0, -1):  # ä»é«˜åˆ°ä½æŸ¥æ‰¾
        candidate = os.path.join(checkpoint_dir, f'epoch_{epoch}.pth')
        if os.path.exists(candidate):
            latest_checkpoint_path = candidate
            break
    
    if latest_checkpoint_path is None:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°å…¶ä»–æ£€æŸ¥ç‚¹ï¼Œä»…è¯„ä¼°æœ€ä½³æ£€æŸ¥ç‚¹")
        results_latest = results_best
    else:
        logger.info(f"\n{'='*80}")
        logger.info(f"è¯„ä¼°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint_path}")
        logger.info(f"{'='*80}")
        
        checkpoint = torch.load(latest_checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"âœ… åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹ (Epoch {checkpoint.get('epoch', 'unknown')})")
        
        results_latest = evaluate_model(
            model, val_loader, device, config, class_names,
            score_thresh=0.3, logger=logger
        )
        logger.info(f"æœ€æ–°æ£€æŸ¥ç‚¹ mAP: {results_latest['mAP']:.4f}")
    
    # ç”Ÿæˆå¯è§†åŒ–å’ŒæŠ¥å‘Š
    logger.info("\nç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    visualize_comparison(results_best, results_latest, class_names, output_dir)
    
    logger.info("\nç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    report_file = os.path.join(output_dir, 'evaluation_report.txt')
    generate_report(results_best, results_latest, class_names, report_file, logger)
    
    # ä¿å­˜ç»“æœä¸ºJSON
    results_json = {
        'best_checkpoint': {
            'mAP': float(results_best['mAP']),
            'precision': float(results_best['precision']),
            'recall': float(results_best['recall']),
            'f1': float(results_best['f1']),
            'avg_iou': float(results_best['avg_iou']),
            'total_tp': int(results_best['total_tp']),
            'total_fp': int(results_best['total_fp']),
            'total_fn': int(results_best['total_fn'])
        },
        'latest_checkpoint': {
            'mAP': float(results_latest['mAP']),
            'precision': float(results_latest['precision']),
            'recall': float(results_latest['recall']),
            'f1': float(results_latest['f1']),
            'avg_iou': float(results_latest['avg_iou']),
            'total_tp': int(results_latest['total_tp']),
            'total_fp': int(results_latest['total_fp']),
            'total_fn': int(results_latest['total_fn'])
        }
    }
    
    json_file = os.path.join(output_dir, 'results.json')
    with open(json_file, 'w') as f:
        json.dump(results_json, f, indent=2)
    logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
    
    logger.info("\n" + "="*80)
    logger.info("è¯„ä¼°å®Œæˆï¼")
    logger.info("="*80)
    
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/")
    print(f"  - è¯„ä¼°æŠ¥å‘Š: {report_file}")
    print(f"  - å¯¹æ¯”å›¾è¡¨: {os.path.join(output_dir, 'comparison_charts.png')}")
    print(f"  - JSON ç»“æœ: {json_file}")
    print(f"  - æ—¥å¿—æ–‡ä»¶: {log_file}")


if __name__ == "__main__":
    main()

