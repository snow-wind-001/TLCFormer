"""
DeepSpeed Multi-GPU Training for OSFormer
ä½¿ç”¨ZeRO-2ä¼˜åŒ–çš„4å¡è®­ç»ƒ
åŸºäºå•å¡ç‰ˆæœ¬ train_rgbb_tiny.py æ”¹é€ 
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from datetime import datetime
import logging
from tqdm import tqdm
import json
import deepspeed
from deepspeed.utils import RepeatingLoader

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from datasets.rgbt_tiny_coco import RGBTTinyCOCODataset, collate_fn
from models.osformer import build_osformer
from utils.loss import compute_loss
from utils.target_utils import convert_targets_for_loss
from utils.visualize import visualize_detection_results, images_to_tensorboard_grid


def setup_logging(log_dir, rank):
    """è®¾ç½®æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹è®°å½•ï¼‰"""
    os.makedirs(log_dir, exist_ok=True)
    
    if rank == 0:
        log_file = os.path.join(log_dir, f'train_{datetime.now():%Y%m%d_%H%M%S}.log')
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    else:
        # éä¸»è¿›ç¨‹åªè¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œçº§åˆ«ä¸ºWARNING
        logging.basicConfig(
            level=logging.WARNING,
            format='%(asctime)s - [Rank {}] - %(levelname)s - %(message)s'.format(rank),
            handlers=[logging.StreamHandler()]
        )
    
    return logging.getLogger(__name__)


def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config


def build_datasets(config):
    """æ„å»ºæ•°æ®é›†"""
    train_dataset = RGBTTinyCOCODataset(
        root_dir=config['data']['root_dir'],
        split='train',
        num_frames=config['model']['num_frames'],
        img_size=config['model']['img_size'],
        modality=config['data'].get('modality', 'both')
    )
    
    val_dataset = RGBTTinyCOCODataset(
        root_dir=config['data']['root_dir'],
        split='test',
        num_frames=config['model']['num_frames'],
        img_size=config['model']['img_size'],
        modality=config['data'].get('modality', 'both')
    )
    
    return train_dataset, val_dataset


def build_dataloaders(train_dataset, val_dataset, config, rank, world_size):
    """æ„å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒåˆ†å¸ƒå¼ï¼‰"""
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    
    val_sampler = torch.utils.data.distributed.DistributedSampler(
        val_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=False
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['train']['batch_size'],
        sampler=train_sampler,
        num_workers=config['data']['num_workers'],
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['train']['batch_size'],
        sampler=val_sampler,
        num_workers=config['data']['num_workers'],
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=False
    )
    
    return train_loader, val_loader, train_sampler, val_sampler


def build_model(config):
    """æ„å»ºæ¨¡å‹"""
    model = build_osformer(
        num_classes=config['model']['num_classes'],
        num_frames=config['model']['num_frames'],
        img_size=config['model']['img_size'],
        use_doppler=config['model']['use_doppler']
    )
    return model


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=15, min_delta=0.001, mode='max'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
    
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == 'max':
            improved = (score - self.best_score) > self.min_delta
        else:
            improved = (self.best_score - score) > self.min_delta
        
        if improved:
            self.best_score = score
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience


def train_one_epoch(model_engine, train_loader, device, config, epoch, writer, rank, world_size):
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆDeepSpeedç‰ˆæœ¬ï¼‰"""
    model_engine.train()
    
    total_loss = 0.0
    loss_components = {
        'cls_loss': 0.0,
        'bbox_loss': 0.0,
        'centerness_loss': 0.0,
        'offset_loss': 0.0
    }
    
    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºè¿›åº¦æ¡
    if rank == 0:
        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')
    else:
        pbar = train_loader
    
    for batch_idx, batch in enumerate(pbar):
        # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        rgb = batch['rgb'].to(device)
        thermal = batch['thermal'].to(device)
        targets_batch = batch['targets']
        
        # å‰å‘ä¼ æ’­
        predictions = model_engine(rgb, thermal)
        
        # è½¬æ¢ç›®æ ‡æ ¼å¼
        feature_size = config['model']['img_size'] // 16
        targets = convert_targets_for_loss(
            targets_batch,
            num_frames=config['model']['num_frames'],
            img_size=config['model']['img_size'],
            feature_size=feature_size,
            device=device
        )
        
        # è®¡ç®—æŸå¤±
        loss, loss_dict = compute_loss(
            predictions, targets, config['train']['loss_weights']
        )
        
        # DeepSpeed backward
        model_engine.backward(loss)
        
        # DeepSpeed step
        model_engine.step()
        
        # ç´¯ç§¯æŸå¤±
        total_loss += loss.item()
        for key in loss_components:
            if key in loss_dict:
                loss_components[key] += loss_dict[key]
        
        # è®°å½•åˆ° TensorBoard (åªåœ¨ä¸»è¿›ç¨‹)
        if rank == 0 and writer is not None:
            global_step = epoch * len(train_loader) + batch_idx
            writer.add_scalar('train/batch_loss', loss.item(), global_step)
            writer.add_scalar('train/learning_rate', model_engine.get_lr()[0], global_step)
            
            # æ¯100ä¸ªbatchè®°å½•æŸå¤±åˆ†é‡
            if batch_idx % 100 == 0:
                writer.add_scalar('train/batch_cls_loss', loss_dict['cls_loss'], global_step)
                writer.add_scalar('train/batch_bbox_loss', loss_dict['bbox_loss'], global_step)
                writer.add_scalar('train/batch_centerness_loss', loss_dict['centerness_loss'], global_step)
                writer.add_scalar('train/batch_offset_loss', loss_dict['offset_loss'], global_step)
        
        # æ›´æ–°è¿›åº¦æ¡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if rank == 0:
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'lr': f'{model_engine.get_lr()[0]:.6f}'
            })
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / len(train_loader)
    avg_components = {k: v / len(train_loader) for k, v in loss_components.items()}
    
    return avg_loss, avg_components


def save_checkpoint(model_engine, epoch, loss, config, save_path, rank):
    """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆDeepSpeedï¼‰"""
    if rank == 0:
        # DeepSpeedä¿å­˜
        model_engine.save_checkpoint(
            save_dir=os.path.dirname(save_path),
            tag=f'epoch_{epoch}',
            client_state={'loss': loss, 'config': config}
        )
        print(f'Checkpoint saved: epoch_{epoch}')


def find_best_checkpoint(checkpoint_dir):
    """è‡ªåŠ¨æŸ¥æ‰¾æœ€ä½³æ¨¡å‹"""
    best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')
    if os.path.exists(best_model_path):
        return best_model_path
    return None


def find_latest_checkpoint(checkpoint_dir):
    """è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°checkpoint"""
    import glob
    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'epoch_*.pth'))
    if not checkpoint_files:
        return None
    
    def extract_epoch(path):
        basename = os.path.basename(path)
        epoch_str = basename.replace('epoch_', '').replace('.pth', '')
        try:
            return int(epoch_str)
        except:
            return -1
    
    checkpoint_files.sort(key=extract_epoch, reverse=True)
    return checkpoint_files[0] if checkpoint_files else None


def main():
    parser = argparse.ArgumentParser(description='Train OSFormer with DeepSpeed (Multi-GPU)')
    
    # ========== ä¸å•å¡è®­ç»ƒå®Œå…¨å…¼å®¹çš„å‚æ•° ==========
    parser.add_argument('--config', type=str,
                       default='./configs/rgbt_tiny_config.yaml',
                       help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from. Special values: "best", "latest", "auto"')
    parser.add_argument('--resume_from_best', action='store_true',
                       help='Automatically resume from best_model.pth')
    parser.add_argument('--resume_from_latest', action='store_true',
                       help='Automatically resume from latest epoch checkpoint')
    parser.add_argument('--reset_optimizer', action='store_true',
                       help='Reset optimizer when resuming (for fine-tuning)')
    parser.add_argument('--reset_epochs', action='store_true',
                       help='Reset epoch counter to 0 when resuming')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Number of epochs (override config)')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='Batch size per GPU (override config)')
    parser.add_argument('--lr', type=float, default=None,
                       help='Learning rate (override config)')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (ignored in DeepSpeed, kept for compatibility)')
    parser.add_argument('--amp', action='store_true',
                       help='Use mixed precision training (automatically enabled in DeepSpeed)')
    
    # ========== DeepSpeedç‰¹å®šå‚æ•° ==========
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='Local rank for distributed training (auto-set by DeepSpeed)')
    
    # æ·»åŠ DeepSpeedå‚æ•°ï¼ˆDeepSpeedä¼šè‡ªåŠ¨æ·»åŠ --deepspeed_configï¼‰
    parser = deepspeed.add_config_arguments(parser)
    
    # è®¾ç½®deepspeed_configçš„é»˜è®¤å€¼ï¼ˆå¦‚æœæ²¡æœ‰é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šï¼‰
    # æ³¨æ„ï¼šè¿™ä¸ªå‚æ•°ç”±deepspeed.add_config_arguments()æ·»åŠ 
    
    args = parser.parse_args()
    
    # è®¾ç½®deepspeed_configé»˜è®¤å€¼ï¼ˆå¦‚æœæ²¡æœ‰æŒ‡å®šï¼‰
    if args.deepspeed_config is None:
        args.deepspeed_config = './configs/deepspeed_config.json'
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # ========== å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®ï¼ˆä¸å•å¡è®­ç»ƒå®Œå…¨ä¸€è‡´ï¼‰ ==========
    if args.epochs:
        config['train']['num_epochs'] = args.epochs
    if args.batch_size:
        config['train']['batch_size'] = args.batch_size
    if args.lr:
        # æ›´æ–°DeepSpeedé…ç½®ä¸­çš„å­¦ä¹ ç‡
        if os.path.exists(args.deepspeed_config):
            with open(args.deepspeed_config, 'r') as f:
                ds_config = json.load(f)
            ds_config['optimizer']['params']['lr'] = args.lr
            ds_config['scheduler']['params']['warmup_max_lr'] = args.lr
            # ä¸´æ—¶ä¿å­˜ä¿®æ”¹åçš„é…ç½®
            temp_ds_config = args.deepspeed_config.replace('.json', '_temp.json')
            with open(temp_ds_config, 'w') as f:
                json.dump(ds_config, f, indent=2)
            args.deepspeed_config = temp_ds_config
    
    # åˆå§‹åŒ–DeepSpeed
    deepspeed.init_distributed()
    
    # è·å–rankå’Œworld_size
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{local_rank}')
    torch.cuda.set_device(device)
    
    # è®¾ç½®æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹è¯¦ç»†è®°å½•ï¼‰
    logger = setup_logging(config['save']['log_dir'], rank)
    
    if rank == 0:
        logger.info(f'Starting DeepSpeed training with {world_size} GPUs')
        logger.info(f'Config: {args.config}')
        logger.info(f'DeepSpeed config: {args.deepspeed_config}')
    
    # æ„å»ºæ•°æ®é›†
    if rank == 0:
        logger.info('Building datasets...')
    train_dataset, val_dataset = build_datasets(config)
    
    if rank == 0:
        logger.info(f'Train dataset: {len(train_dataset)} samples')
        logger.info(f'Val dataset: {len(val_dataset)} samples')
        logger.info(f'Per GPU batch size: {config["train"]["batch_size"]}')
        logger.info(f'Global batch size: {config["train"]["batch_size"] * world_size}')
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, train_sampler, val_sampler = build_dataloaders(
        train_dataset, val_dataset, config, rank, world_size
    )
    
    # æ„å»ºæ¨¡å‹
    if rank == 0:
        logger.info('Building model...')
    model = build_model(config)
    
    # è®¡ç®—å‚æ•°é‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if rank == 0:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.info(f'Model parameters: {total_params:,} total, {trainable_params:,} trainable')
    
    # ==================== åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€å’ŒResumeé€»è¾‘ ====================
    start_epoch = 0
    best_metric_name = config['save'].get('best_metric', 'map50')
    best_epoch = 0
    
    # åˆå§‹åŒ– best_metric
    if best_metric_name in ['loss', 'val_loss']:
        best_metric = float('inf')
        best_mode = 'min'
    else:
        best_metric = 0.0
        best_mode = 'max'
    
    if rank == 0:
        logger.info(f'Model selection metric: {best_metric_name} (mode: {best_mode})')
    
    # ==================== æ™ºèƒ½æ¢å¤è®­ç»ƒï¼ˆä¸å•å¡å®Œå…¨ä¸€è‡´ï¼‰ ====================
    resume_path = None
    load_info = None  # åˆå§‹åŒ–load_info
    
    # ç¡®å®šè¦æ¢å¤çš„checkpointè·¯å¾„
    if args.resume_from_best:
        resume_path = find_best_checkpoint(config['save']['checkpoint_dir'])
        if resume_path and rank == 0:
            logger.info(f'ğŸ” Found best model: {resume_path}')
        elif rank == 0:
            logger.warning('âŒ No best_model.pth found')
    
    elif args.resume_from_latest:
        resume_path = find_latest_checkpoint(config['save']['checkpoint_dir'])
        if resume_path and rank == 0:
            logger.info(f'ğŸ” Found latest checkpoint: {resume_path}')
        elif rank == 0:
            logger.warning('âŒ No epoch checkpoints found')
    
    elif args.resume:
        # å¤„ç†ç‰¹æ®Šå€¼
        if args.resume.lower() == 'best':
            resume_path = find_best_checkpoint(config['save']['checkpoint_dir'])
        elif args.resume.lower() == 'latest':
            resume_path = find_latest_checkpoint(config['save']['checkpoint_dir'])
        elif args.resume.lower() == 'auto':
            resume_path = find_best_checkpoint(config['save']['checkpoint_dir'])
            if not resume_path:
                resume_path = find_latest_checkpoint(config['save']['checkpoint_dir'])
            if resume_path and rank == 0:
                logger.info(f'ğŸ” Auto-selected checkpoint: {resume_path}')
        else:
            resume_path = args.resume
    
    # åŠ è½½checkpointï¼ˆåœ¨DeepSpeedåˆå§‹åŒ–å‰ï¼‰
    if resume_path and os.path.exists(resume_path):
        if rank == 0:
            logger.info('=' * 60)
            logger.info('ğŸ“¥ LOADING CHECKPOINT')
            logger.info('=' * 60)
        
        try:
            checkpoint = torch.load(resume_path, map_location='cpu')
            
            # æå–ä¿¡æ¯ä½†ä¸ç«‹å³åŠ è½½ï¼ˆDeepSpeedä¼šå¤„ç†æ¨¡å‹æƒé‡ï¼‰
            if not args.reset_epochs:
                start_epoch = checkpoint.get('epoch', 0) + 1
                if rank == 0:
                    logger.info(f'âœ… Will resume from epoch {start_epoch}')
            else:
                start_epoch = 0
                if rank == 0:
                    logger.info('âš ï¸  Epoch counter reset to 0 (fine-tuning mode)')
            
            # åŠ è½½æœ€ä½³æŒ‡æ ‡ä¿¡æ¯
            if 'loss' in checkpoint and isinstance(checkpoint['loss'], dict):
                loss_info = checkpoint['loss']
                best_metric = loss_info.get('best_metric', best_metric)
                best_epoch = loss_info.get('best_epoch', 0)
                
                if rank == 0:
                    logger.info(f'âœ… Best metric: {best_metric_name}={best_metric:.4f} (epoch {best_epoch})')
            
            load_info = checkpoint
            
            if rank == 0:
                logger.info('=' * 60)
        
        except Exception as e:
            if rank == 0:
                logger.error(f'âŒ Failed to load checkpoint: {e}')
                import traceback
                traceback.print_exc()
    
    # åˆå§‹åŒ–DeepSpeed engine
    model_engine, optimizer, _, _ = deepspeed.initialize(
        args=args,
        model=model,
        model_parameters=model.parameters()
    )
    
    # åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå¦‚æœæœ‰checkpointï¼‰
    if load_info is not None:
        if 'model_state_dict' in load_info:
            # åŠ è½½æ¨¡å‹æƒé‡
            model_engine.load_state_dict(load_info['model_state_dict'], strict=True)
            if rank == 0:
                logger.info('âœ… Model weights loaded')
            
            # åŠ è½½ä¼˜åŒ–å™¨ï¼ˆå¦‚æœä¸resetï¼‰
            if not args.reset_optimizer and 'optimizer_state_dict' in load_info:
                try:
                    optimizer.load_state_dict(load_info['optimizer_state_dict'])
                    if rank == 0:
                        logger.info('âœ… Optimizer state loaded')
                except Exception as e:
                    if rank == 0:
                        logger.warning(f'âš ï¸  Failed to load optimizer state: {e}')
            elif args.reset_optimizer and rank == 0:
                logger.info('âš ï¸  Optimizer reset (fine-tuning mode)')
    
    # åˆ›å»º TensorBoard writer (åªåœ¨ä¸»è¿›ç¨‹)
    writer = None
    if rank == 0:
        tensorboard_dir = config['save'].get('tensorboard_dir', './runs/rgbt_tiny_deepspeed')
        os.makedirs(tensorboard_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=tensorboard_dir)
        logger.info(f'TensorBoard logs: {tensorboard_dir}')
        
        # è®°å½•é…ç½®
        writer.add_text('config/training', f'DeepSpeed ZeRO-2, {world_size} GPUs', 0)
        writer.add_text('config/loss_function', 'CIoU Loss', 0)
        writer.add_text('config/loss_weights', str(config['train']['loss_weights']), 0)
    
    # åˆå§‹åŒ–æ—©åœæœºåˆ¶ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    early_stopping = None
    if rank == 0 and config.get('early_stopping', {}).get('enabled', False):
        early_stopping = EarlyStopping(
            patience=config['early_stopping'].get('patience', 15),
            min_delta=config['early_stopping'].get('min_delta', 0.001),
            mode=config['early_stopping'].get('mode', 'max')
        )
        logger.info(f'Early stopping enabled with patience={early_stopping.patience}')
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    if rank == 0:
        os.makedirs(config['save']['checkpoint_dir'], exist_ok=True)
    
    # è®­ç»ƒå¾ªç¯
    if rank == 0:
        logger.info('Starting training...')
    
    for epoch in range(start_epoch, config['train']['num_epochs']):
        # è®¾ç½®epochï¼ˆç”¨äºåˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼‰
        train_sampler.set_epoch(epoch)
        
        if rank == 0:
            logger.info(f'Epoch {epoch}/{config["train"]["num_epochs"]}')
        
        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss, loss_components = train_one_epoch(
            model_engine, train_loader, device, config, epoch, writer, rank, world_size
        )
        
        if rank == 0:
            logger.info(f'Train loss: {train_loss:.4f}')
            logger.info(f'Loss components: {loss_components}')
            
            # è®°å½•è®­ç»ƒæŸå¤±åˆ° TensorBoard
            writer.add_scalar('epoch/train_loss', train_loss, epoch)
            for key, value in loss_components.items():
                writer.add_scalar(f'epoch/train_{key}', value, epoch)
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if rank == 0 and (epoch + 1) % config['save']['save_interval'] == 0:
            checkpoint_path = os.path.join(
                config['save']['checkpoint_dir'], f'epoch_{epoch}.pth'
            )
            save_checkpoint(
                model_engine, epoch,
                {'train_loss': train_loss, 'loss_components': loss_components},
                config, checkpoint_path, rank
            )
            logger.info(f'ğŸ’¾ Checkpoint saved: epoch_{epoch}')
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        torch.distributed.barrier()
    
    # è®­ç»ƒç»“æŸ
    if rank == 0:
        writer.close()
        logger.info('=' * 60)
        logger.info('ğŸ‰ TRAINING COMPLETED!')
        logger.info('=' * 60)
        logger.info(f'Total Epochs: {config["train"]["num_epochs"]}')
        logger.info(f'Checkpoints: {config["save"]["checkpoint_dir"]}')
        logger.info('=' * 60)
    
    # æ¸…ç†
    deepspeed.sys.exit()


if __name__ == '__main__':
    main()

